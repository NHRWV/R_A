View(tweets)
# display main body(text) of tweet
tweets[[1]]$text
# display favorite count
tweets[[1]]$favoriteCount
# check current rate limit (Check /statuses/user_timeline)
getCurRateLimitInfo()
# extract a few sample tweets from this user's timeline
tweets <- userTimeline(twitterUser, n = 10)
# display attributes and function of tweet object
tweets[[1]]$getClass()
# display main body(text) of tweet
tweets[[1]]$text
# display favorite count
tweets[[1]]$favoriteCount
# check current rate limit (Check /statuses/user_timeline)
getCurRateLimitInfo()
tweets <- userTimeline(twitterUser, n = 10, includeRts = T)
# display main body(text) of tweet
tweets[[1]]$text
# display favorite count
tweets[[1]]$favoriteCount
#install.packages("tm")
library(tm)
#install.packages("ggmap")
library(ggmap)
library(ggplot2)
library(twitteR)
#ibrary(stringr)
#install.packages("wordcloud")
library(wordcloud)
#install.packages("lubridate")
library(lubridate)
library(data.table)
# set the credentials (just in case)
CONSUMER_SECRET <- "E8u5PZvaNvCNXIi34yEE8Efs8AEz2TxJbvyjpjWi2OpE3HENDQ"
CONSUMER_KEY <- "hvHT3aR6Reugq4OMSEUG0sums"
ACCESS_SECRET <- "BbKb4BFRfzyVppA380bh5FHxEjgV2s0PefdNy7Avthmsy"
ACCESS_TOKEN <- "701807611465740288-g0Dp8IPrCv8VTEJfKRStR1dGHONeoxN"
# plot by source
# encode tweet source as iPhone, iPad, Android or Web
# gsub (global substitute): Replace 1st arg with 2nd arg in 3rd arg string
encodeSource <- function(x) {
if(grepl(">Twitter for iPhone</a>", x)){
"iphone"
}else if(grepl(">Twitter for iPad</a>", x)){
"ipad"
}else if(grepl(">Twitter for Android</a>", x)){
"android"
} else if(grepl(">Twitter Web Client</a>", x)){
"Web"
} else if(grepl(">Twitter for Windows Phone</a>", x)){
"windows phone"
}else if(grepl(">dlvr.it</a>", x)){
"dlvr.it"
}else if(grepl(">IFTTT</a>", x)){
"ifttt"
}else if(grepl(">EarthquakeTrack.com</a>", x)){
"earthquaketrack"
}else if(grepl(">Did You Feel It</a>", x)){
"did_you_feel_it"
}else if(grepl(">Earthquake Mobile</a>", x)){
"earthquake_mobile"
}else if(grepl(">Facebook</a>", x)){  #This looks unreliable...
"facebook"
}else {
"others"
}
}
# connect to twitter app
setup_twitter_oauth(consumer_key = CONSUMER_KEY,
consumer_secret = CONSUMER_SECRET,
access_token = ACCESS_TOKEN,
access_secret = ACCESS_SECRET)
# extract tweets based on a search term
searchTerm <- "#earthquake"
trendingTweets = searchTwitter(searchTerm,n=1000,lang = "en")
#even with lang option, mapCountry gives an error... due to emoji?
class(trendingTweets)
head(trendingTweets)
str(trendingTweets[[1]])
# perform a quick cleanup/transformation
trendingTweets.df = twListToDF(trendingTweets)
class(trendingTweets.df)
library(twitteR)
# library(data.table)
library(tm)
library(ggplot2)
library(stringr)
#install.packages("syuzhet")
library(syuzhet)
library(wordcloud)
library(RColorBrewer)
# set the credentials
CONSUMER_SECRET <- "E8u5PZvaNvCNXIi34yEE8Efs8AEz2TxJbvyjpjWi2OpE3HENDQ"
CONSUMER_KEY <- "hvHT3aR6Reugq4OMSEUG0sums"
ACCESS_SECRET <- "BbKb4BFRfzyVppA380bh5FHxEjgV2s0PefdNy7Avthmsy"
ACCESS_TOKEN <- "701807611465740288-g0Dp8IPrCv8VTEJfKRStR1dGHONeoxN"
#extract timeline tweets
extractTimelineTweets <- function(username,tweetCount){
# timeline tweets
twitterUser <- getUser(username)
tweets = userTimeline(twitterUser,n=tweetCount)
tweets.df = twListToDF(tweets)
#tweets.df$text <- sapply(tweets.df$text,function(x) iconv(x,to='UTF-8'))  #commented out due to missing text
tweets.df$text <- sapply(tweets.df$text,function(x) iconv(enc2utf8(x), sub="byte"))
return(tweets.df)
}
encodeSentiment <- function(x) {
if(x <= -0.5){
"1) very negative"
}else if(x > -0.5 & x < 0){
"2) negative"
}else if(x > 0 & x < 0.5){
"4) positive"
}else if(x >= 0.5){
"5) very positive"
}else {
"3) neutral"
}
}
# connect to twitter app
setup_twitter_oauth(consumer_key = CONSUMER_KEY,
consumer_secret = CONSUMER_SECRET,
access_token = ACCESS_TOKEN,
access_secret = ACCESS_SECRET)
tweetsDF <- extractTimelineTweets("realDonaldTrump",3200)  #POTUS doesn't work
View(tweetsDF)
nohandles <- str_replace_all(tweetsDF$text, "@\\w+", "") # @뒤의 문자..그러니까 계정명을 제거.
# 트위터에서 언급하는 계정을 삭제하는 것이다.
wordCorpus <- Corpus(VectorSource(nohandles))
wordCorpus[[1]]$content
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus[[1]]$content
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus[[1]]$content
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english")) # 불용어처리
wordCorpus[[1]]$content
wordCorpus <- tm_map(wordCorpus, removeWords, c("amp"))  #manual assignment
wordCorpus[[1]]$content
wordCorpus <- tm_map(wordCorpus, stripWhitespace) # 긴 공백들 전부 빈칸 하나로 바꾸기
wordCorpus[[1]]$content
str(wordCorpus)
pal <- brewer.pal(9,"YlGnBu")
pal <- pal[-(1:4)]
set.seed(123)
wordcloud(words = wordCorpus, scale=c(5,0.1), max.words=1000, random.order=FALSE,
rot.per=0.35, use.r.layout=FALSE, colors=pal)
tweetSentiments <- get_sentiment (tweetsDF$text,method = "syuzhet")
tweets <- cbind(tweetsDF, tweetSentiments)
tweets$sentiment <- sapply(tweets$tweetSentiments,encodeSentiment)
head(tweets, n = 1)
names(tweets)
qplot(tweets$tweetSentiments) + theme(legend.position="none")+
xlab("Sentiment Score") +
ylab("Number of tweets") +
ggtitle("Tweets by Sentiment Score")
ggplot(tweets, aes(sentiment)) +
geom_bar(fill = "aquamarine4") +
theme(legend.position="none", axis.title.x = element_blank()) +
ylab("Number of tweets") +
ggtitle("Tweets by Sentiment")
tweetSentiments <- get_nrc_sentiment(tweetsDF$text)
tweets <- cbind(tweetsDF, tweetSentiments)
tweets[c(1:5),c(1, 17:26)]
sentimentTotals <- data.frame(colSums(tweets[,c(17:26)]))
names(sentimentTotals) <- "count"
sentimentTotals <- cbind("sentiment" = rownames(sentimentTotals), sentimentTotals)
rownames(sentimentTotals) <- NULL
sentimentTotals
ggplot(data = sentimentTotals, aes(x = sentiment, y = count)) +
geom_bar(aes(fill = sentiment), stat = "identity") +
theme(legend.position = "none") +
xlab("Sentiment") + ylab("Total Count") + ggtitle("Total Sentiment Score for All Tweets")
# NRC Sample (various emotions such as anger, fear, joy, ...)
tweetSentiments <- get_nrc_sentiment(tweetsDF$text)
head(tweetSentiments)
tweetsDF$text(5)
tweetsDF$text(5)
tweets <- cbind(tweetsDF, tweetSentiments)
tweets[c(1:5),c(1, 17:26)]
names(sentimentTotals) <- "count"
sentimentTotals <- cbind("sentiment" = rownames(sentimentTotals), sentimentTotals)
rownames(sentimentTotals) <- NULL
sentimentTotals
install.packages("igraph")
# set the credentials (just in case)
CONSUMER_SECRET <- "E8u5PZvaNvCNXIi34yEE8Efs8AEz2TxJbvyjpjWi2OpE3HENDQ"
CONSUMER_KEY <- "hvHT3aR6Reugq4OMSEUG0sums"
ACCESS_SECRET <- "BbKb4BFRfzyVppA380bh5FHxEjgV2s0PefdNy7Avthmsy"
ACCESS_TOKEN <- "701807611465740288-g0Dp8IPrCv8VTEJfKRStR1dGHONeoxN"
# get follower names
get_follower_list <- function(userName){
# get user data
twitterUser <- getUser(userName)
twitterUserFollowerIDs<-twitterUser$getFollowers(retryOnRateLimit=1)
# extract the list of followers
return (sapply(twitterUserFollowerIDs,screenName))
}
# append rows to dataframe
append_to_df<-function(dt, elems)
{
return(rbindlist(list(dt,  elems),use.names = TRUE))
}
############################################################################
#             Follower Graph Analysis
############################################################################
# connect to twitter app
setup_twitter_oauth(consumer_key = CONSUMER_KEY,
consumer_secret = CONSUMER_SECRET,
access_token = ACCESS_TOKEN,
access_secret = ACCESS_SECRET)
# Begin with a certain username
#coreUserName <- "jack"  #more than 4M followers... too big for an exercise
coreUserName <- "Ajou_University"
twitterUser <- getUser(coreUserName)
# Extract Followers for the core user
twitterUser_follower_IDs <- twitterUser$getFollowers(retryOnRateLimit=10)
str(twitterUser_follower_IDs[1])
twitterUser_followers_df = rbindlist(lapply(
twitterUser_follower_IDs,as.data.frame
))
str(twitterUser_followers_df)
str(twitterUser_followers_df[1])
twitterUser_followers_df = rbindlist(lapply(
twitterUser_follower_IDs,as.data.frame
))
(data.table)
library(twitteR)
library(data.table)
library(igraph)
library(RColorBrewer)
twitterUser_followers_df = rbindlist(lapply(
twitterUser_follower_IDs,as.data.frame
))
str(twitterUser_followers_df)
str(twitterUser_followers_df[1])
# filter dummy accounts (and reduce the number of followers for performance)
filtered_df <- subset(twitterUser_followers_df,
followersCount < 100 &
followersCount > 50 &
#statusesCount > 10000 & #to reduce number of followers
# statusesCount > 100 &
# statusesCount < 5000 & #too many tweets from bots?
protected==FALSE)
filtered_follower_IDs <- filtered_df$screenName
length(filtered_follower_IDs)
# prepare edge data frame (edges to coreUserName)
edge_df<-data.frame(from=filtered_follower_IDs,
to=rep(coreUserName,
length(filtered_follower_IDs)),
stringsAsFactors=FALSE)
head(edge_df)
load("edge_df.Rda")
save(edge_df, file = "edge_df.Rda")
load("edge_df.Rda")
#net <- graph.data.frame(edge_df, directed=T)  #same with graph_from_data_frame()
net <- graph_from_data_frame(edge_df, directed=T)
table(edge_df$to)
edge_df[to=="Ajou_University"]$from
#net <- graph.data.frame(edge_df, directed=T)  #same with graph_from_data_frame()
net <- graph_from_data_frame(edge_df, directed=T)
table(edge_df$to)
edge_df[to=="Ajou_University"]$from
edge_df[from=="Ajou_University"]$to
# adjust the size of nodes based on in and out degrees
deg <- degree(net, mode="all")
V(net)$size <- deg*0.05 + 1
V(net)[name == coreUserName]$size <- 15
V(net)[size >= 15]$name
V(net)[name == coreUserName]$size
# node coloring
pal3 <- brewer.pal(10, "Set3")
# overall follower graph
plot(net, edge.arrow.size=0.1,
#vertex.label = ifelse(V(net)$size >= 15, V(net)$name, NA),
vertex.label = ifelse(V(net)$size >= 5, V(net)$name, NA),
vertex.color = pal3)
# Plot to highlight Followers with large number of followers
deg <- degree(net, mode="out")
V(net)$size <- deg*0.05+2
V(net)[size==max(V(net)$size)]  #the most ties
# Highlight the coreUser
V(net)[coreUserName]$size <- 15
# identify friend vertices (the vertices coreUserName is also following)
friendVertices <- ends(net, es=E(net)[from(coreUserName)])[,2]   #ends finds vertices at the ends of edges
ends(net, es=E(net)[from(coreUserName)])[,2]
# Generate edge color variable: (normal: grey80, friend: red)
ecol <- rep("grey80", ecount(net))
ecol[which (V(net)$name %in% friendVertices)] <- 'red'
# add core_user for vertex coloring
friendVertices <- append(friendVertices,coreUserName)
# Generate node color variable: (normal: grey80, friend & coreUser: gold)
vcol <- rep("grey80", vcount(net))
vcol[which (V(net)$name %in% friendVertices)] <- "gold"
# vertex label size
V(net)$label.cex <- 1.2
plot(net,
vertex.color=vcol,
edge.color=ecol,
edge.width=ew,
edge.arrow.mode=0,
vertex.label = ifelse(V(net)$name %in% friendVertices, V(net)$name, NA),
vertex.label.color="black",
vertex.label.font=2,
edge.curved=0.1
)
library(twitteR)
# set the credentials (just in case)
CONSUMER_SECRET <- "E8u5PZvaNvCNXIi34yEE8Efs8AEz2TxJbvyjpjWi2OpE3HENDQ"
CONSUMER_KEY <- "hvHT3aR6Reugq4OMSEUG0sums"
ACCESS_SECRET <- "BbKb4BFRfzyVppA380bh5FHxEjgV2s0PefdNy7Avthmsy"
ACCESS_TOKEN <- "701807611465740288-g0Dp8IPrCv8VTEJfKRStR1dGHONeoxN"
# connect to twitter app
setup_twitter_oauth(consumer_key = CONSUMER_KEY,
consumer_secret = CONSUMER_SECRET,
access_token = ACCESS_TOKEN,
access_secret = ACCESS_SECRET)
# extract tweets based on a search term
searchTerm <- "#earthquake"
trendingTweets = searchTwitter(searchTerm,n=1000,lang = "en")
class(trendingTweets)
head(trendingTweets)
str(trendingTweets[[1]])
# perform a quick cleanup/transformation
trendingTweets.df = twListToDF(trendingTweets)
View(trendingTweets.df)
class(trendingTweets.df)
names(trendingTweets.df)
head(trendingTweets.df)
head(trendingTweets.df$text)
#trendingTweets.df$text <- sapply(trendingTweets.df$text,function(x) iconv(x,to='UTF-8')) #Removes some (unicode?) values in text field
trendingTweets.df$text <- sapply(trendingTweets.df$text,function(x) iconv(enc2utf8(x), sub="byte"))  #this works fine!!
head(trendingTweets.df$text)
head(trendingTweets.df$created)
class(trendingTweets.df$created)
save(trendingTweets.df, file = "trendingTweets20190811.Rda")
# see how many missing values are there on a per column basis
sapply(trendingTweets.df, function(x) sum(is.na(x)))
=
# plot on tweets by time
ggplot(data = trendingTweets.df, aes(x = created)) +
geom_histogram(aes(fill = ..count..)) +
theme(legend.position = "none") +
xlab("Time") + ylab("Number of tweets") +
scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
library(ggplot2)
#ibrary(stringr)
#install.packages("wordcloud")
library(wordcloud)
library(data.table)
ggplot(trendingTweets.df[trendingTweets.df$tweetSource != 'others',], aes(tweetSource)) +
geom_bar(fill = "aquamarine4") +
theme(legend.position="none",
axis.title.x = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)) +
ylab("Number of tweets") +
ggtitle("Tweets by Source")
if(grepl(">Twitter for iPhone</a>", x)){
"iphone"
}else if(grepl(">Twitter for iPad</a>", x)){
"ipad"
}else if(grepl(">Twitter for Android</a>", x)){
"android"
} else if(grepl(">Twitter Web Client</a>", x)){
"Web"
} else if(grepl(">Twitter for Windows Phone</a>", x)){
"windows phone"
}else if(grepl(">dlvr.it</a>", x)){
"dlvr.it"
}else if(grepl(">IFTTT</a>", x)){
"ifttt"
}else if(grepl(">EarthquakeTrack.com</a>", x)){
"earthquaketrack"
}else if(grepl(">Did You Feel It</a>", x)){
"did_you_feel_it"
}else if(grepl(">Earthquake Mobile</a>", x)){
"earthquake_mobile"
}else if(grepl(">Facebook</a>", x)){  #This looks unreliable...
"facebook"
}else {
"others"
}
ggplot(trendingTweets.df[trendingTweets.df$tweetSource != 'others',], aes(tweetSource)) +
geom_bar(fill = "aquamarine4") +
theme(legend.position="none",
axis.title.x = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)) +
ylab("Number of tweets") +
ggtitle("Tweets by Source")
if(grepl(">Twitter for iPhone</a>", x)){
"iphone"
}else if(grepl(">Twitter for iPad</a>", x)){
"ipad"
}else if(grepl(">Twitter for Android</a>", x)){
"android"
} else if(grepl(">Twitter Web Client</a>", x)){
"Web"
} else if(grepl(">Twitter for Windows Phone</a>", x)){
"windows phone"
}else if(grepl(">dlvr.it</a>", x)){
"dlvr.it"
}else if(grepl(">IFTTT</a>", x)){
"ifttt"
}else if(grepl(">EarthquakeTrack.com</a>", x)){
"earthquaketrack"
}else if(grepl(">Did You Feel It</a>", x)){
"did_you_feel_it"
}else if(grepl(">Earthquake Mobile</a>", x)){
"earthquake_mobile"
}else if(grepl(">Facebook</a>", x)){  #This looks unreliable...
"facebook"
}else {
"others"
}
encodeSource <- function(x) {
if(grepl(">Twitter for iPhone</a>", x)){
"iphone"
}else if(grepl(">Twitter for iPad</a>", x)){
"ipad"
}else if(grepl(">Twitter for Android</a>", x)){
"android"
} else if(grepl(">Twitter Web Client</a>", x)){
"Web"
} else if(grepl(">Twitter for Windows Phone</a>", x)){
"windows phone"
}else if(grepl(">dlvr.it</a>", x)){
"dlvr.it"
}else if(grepl(">IFTTT</a>", x)){
"ifttt"
}else if(grepl(">EarthquakeTrack.com</a>", x)){
"earthquaketrack"
}else if(grepl(">Did You Feel It</a>", x)){
"did_you_feel_it"
}else if(grepl(">Earthquake Mobile</a>", x)){
"earthquake_mobile"
}else if(grepl(">Facebook</a>", x)){  #This looks unreliable...
"facebook"
}else {
"others"
}
}
setup_twitter_oauth(consumer_key = CONSUMER_KEY,
consumer_secret = CONSUMER_SECRET,
access_token = ACCESS_TOKEN,
access_secret = ACCESS_SECRET)
ggplot(trendingTweets.df[trendingTweets.df$tweetSource != 'others',], aes(tweetSource)) +
geom_bar(fill = "aquamarine4") +
theme(legend.position="none",
axis.title.x = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)) +
ylab("Number of tweets") +
ggtitle("Tweets by Source")
setup_twitter_oauth(consumer_key = CONSUMER_KEY,
consumer_secret = CONSUMER_SECRET,
access_token = ACCESS_TOKEN,
access_secret = ACCESS_SECRET)
# extract tweets based on a search term
searchTerm <- "#earthquake"
trendingTweets = searchTwitter(searchTerm,n=1000,lang = "en")
class(trendingTweets)
head(trendingTweets)
str(trendingTweets[[1]])
# perform a quick cleanup/transformation
trendingTweets.df = twListToDF(trendingTweets)
class(trendingTweets.df)
names(trendingTweets.df)
head(trendingTweets.df$text)
#trendingTweets.df$text <- sapply(trendingTweets.df$text,function(x) iconv(x,to='UTF-8')) #Removes some (unicode?) values in text field
trendingTweets.df$text <- sapply(trendingTweets.df$text,function(x) iconv(enc2utf8(x), sub="byte"))  #this works fine!!
# see how many missing values are there on a per column basis
sapply(trendingTweets.df, function(x) sum(is.na(x)))
# plot on tweets by time
ggplot(data = trendingTweets.df, aes(x = created)) +
geom_histogram(aes(fill = ..count..)) +
theme(legend.position = "none") +
xlab("Time") + ylab("Number of tweets") +
scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
# plot tweets by source system (android, iphone, web, etc)
trendingTweets.df$tweetSource = sapply(trendingTweets.df$statusSource, encodeSource)
ggplot(trendingTweets.df[trendingTweets.df$tweetSource != 'others',], aes(tweetSource)) +
geom_bar(fill = "aquamarine4") +
theme(legend.position="none",
axis.title.x = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)) +
ylab("Number of tweets") +
ggtitle("Tweets by Source")
namesCorpus <- Corpus(VectorSource(trendingTweets.df$screenName))  #using ScreenName
class(trendingTweets.df$screenName)
str(namesCorpus)
namesCorpus <- Corpus(VectorSource(trendingTweets.df$screenName))  #using ScreenName
str(namesCorpus)
namesCorpus <- Corpus(VectorSource(trendingTweets.df$screenName))
(tm)
#install.packages("tm")
library(tm)
#install.packages("ggmap")
library(ggmap)
#ibrary(stringr)
#install.packages("wordcloud")
library(wordcloud)
#install.packages("lubridate")
library(lubridate)
library(data.table)
namesCorpus <- Corpus(VectorSource(trendingTweets.df$screenName))  #using ScreenName
str(namesCorpus)
pal <- brewer.pal(9,"YlGnBu")
pal <- pal[-(1:4)]
set.seed(42)
par(mar = c(0,0,0,0), mfrow = c(1,1)) # 이거 모르겠네..마진 설정임.
wordcloud(words = namesCorpus, scale=c(3,0.5), max.words=100, random.order=FALSE,
rot.per=0.10, use.r.layout=TRUE, colors=pal)
